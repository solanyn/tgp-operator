apiVersion: v1
kind: Pod
metadata:
  name: test-tgp-gpu-workload
  namespace: default
  labels:
    app: test-tgp-gpu
    gpu-workload: "true"
  annotations:
    tgp.io/vendor: "nvidia"
    tgp.io/workload: "inference"
spec:
  restartPolicy: Never
  containers:
  - name: gpu-test
    image: nvidia/cuda:12.0-runtime-ubuntu20.04
    command: ["bash", "-c"]
    args:
      - |
        echo "Starting TGP GPU test workload..."
        echo "Node: $(hostname)"
        echo "GPU Info:"
        nvidia-smi || echo "nvidia-smi not available"
        echo "Sleeping for 30 minutes to test provisioning..."
        sleep 1800
        echo "GPU test completed"
    resources:
      requests:
        tgp.io/gpu: 1
        tgp.io/memory: "2Gi"
        memory: "4Gi"
        cpu: "2"
      limits:
        tgp.io/gpu: 1
        tgp.io/memory: "2Gi"
        memory: "8Gi"
        cpu: "4"